\section*{Summary}

Type systems have been studied extensively in the domain of static complexity analysis, to formalize rules that can describe the relationships between a program and its resource use in terms of time and memory (space). Formal methods such as type systems have the advantage that they are typically proved sound. In this thesis, we explore the challenges of implementing both type checking and type inference for a type system for parallel complexity of message-passing processes introduced by Baillot and Ghyselen \cite{BaillotGhyselen2021} that has until now not been implemented. Such implementations of type checking and type inference, paired with corresponding soundness proofs, enable verification or inference of correct complexity bounds, respectively. The type system builds on sized types to express parametric complexity, combined with input/output types to bound synchronizations on channels. \\

After briefly presenting the variant of the pi-calculus considered by Baillot and Ghyselen as well as a more formal description of parallel complexity, we provide an overview of their type system. An important concept is that of \textit{constraint judgements}. These judgements allow us to compare parametric sizes and complexity bounds under a set of known constraints on otherwise unknown sizes. We show both types and type rules of the type system, as well as present examples of typings for processes.\\

Constructing a type checker for the type system by Baillot and Ghyselen introduces a number of challenges. In creating algorithmic type rules for their type system, a notable challenge is maintaining the subject reduction property of the type system. That is, if we are not careful, the reduction relation of $\pi$-calculus processes may not be type-preserving under our modified type rules. We solve this problem by introducing the idea of \textit{combined complexities} consisting of a set of intersecting parametric complexity bounds. As such, a combined complexity represents the maximum of all the complexities contained within. We also define the accompanying function \textit{basis} that keeps a combined complexity as a minimal set by removing complexities that never contribute to the actual bound. Finally, we prove our algorithmic type rules sound, to this effort proving a weaker subject reduction property. We also show how the type checker may be extended with more practical constructs, and present an example of a parallel merge sort encoding that would then be typable using our modified rules.\\

We next show how constraint judgements may be verified. As these judgements are universally quantified over size variables (referred to as index variables), comparison of sizes and complexity bounds is a partial order. We first limit ourselves to constraint judgements over linear functions that may be verified by reduction to integer programs, or alternatively by over-approximation using linear programming. To increase expressiveness, we then show how we can reduce constraint judgements on monotonic monovariate polynomial functions to linear constraint judgements.\\

% After shortly introducing the work by Baillot and Ghyselen and the variant of the pi-calculus they consider, we focus on so-called \textit{constraint judgements} that are particularly relevant for their type system. We give different interpretations of the judgements to give both a formal and an intuitive understanding. Judgements are first limited to linear judgements and are verified by reducing the problem to an integer programming problem. To increase expressiveness, we then show how we can reduce monotonic monovariate polynomial constraints to linear constraints.\\

% Constructing a type checker for the type system by Baillot and Ghyselen introduces a number of challenges. One such challenge is checking for the existence of a specific substitution needed during type checking of outputs on \textit{servers}. We prove that this is NP-complete by reducing it to the NP-complete 3-SAT problem. To get around this, we limit ourselves to type checking processes with expressions of a particular form and introduce a function \textit{instantiate} that uses a greedy strategy to find substitutions.\\

% Another problem encountered during construction of the type checker is that of soundness of its type rules. More specifically, if we are not careful, reduction of processes checked by the type checker may no longer type check and the subject reduction property is therefore lost. We solve this problem by introducing the idea of \textit{combined complexities} consisting of a set of intersecting parametric complexity bounds. We also define the accompanying function \textit{basis} that keeps a combined complexity as a minimal set. Finally, we introduce algorithmic type rules for the type checker and prove them sound, including subject reduction. We show how an example process corresponding to the sequential merge function can be type checked using our type checker.\\

Our type inference algorithm is constraint-based, and as such generates constraints that enforce premises of the type rules are satisfied, based on a provided process. Once these constraints have been generated, they are reduced to simpler constraints that may be checked using an off-the-shelf SMT solver. As many of the premises in the type rules are universally quantified constraint judgements, our generated constraints contain both existential quantifiers over unknown coefficients and universal quantifiers over index variables. Such constraints are often not solvable using SMT solvers. As such, we perform a number of over-approximations during constraint reduction that put some limitations on which processes we may infer bounds on the parallel complexity for. We implement type inference in Haskell using the Z3 SMT solver. We find that we can bound the parallel complexity of some linear time and many constant time servers in reasonable time.\\

In conclusion, we have implemented type checking and type inference for the type system by Baillot and Ghyselen, and find that we can type check some polynomial and linear time primitive recursive processes, as well as infer precise bounds on the parallel complexity of some linear time processes and many constant time processes. To increase the expressiveness of our type inference algorithm, future work may include relaxing the over-approximations made during constraint reduction, such that they more closely reflect the original constraints, while still being satisfiable in reasonable time. Furthermore, it may be interesting to see how our type inference algorithm extends to \textit{usage} types, as in the type system by Baillot et al. \cite{BaillotEtAl2021}, which is a generalization of the type system by Baillot and Ghyselen that increases the expressiveness and precision.